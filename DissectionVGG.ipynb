{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3vUZ_12wdWZ",
        "outputId": "0abe8082-1582-4a50-86a6-b4e2180aa45c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 146.0/146.0 kB 10.3 MB/s eta 0:00:00\n",
            "Installing collected packages: ninja\n",
            "Successfully installed ninja-1.11.1\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit \n",
        "! pip install transformers \n",
        "pip install ninja 2>> install.log"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-PyYmFuL8EO0"
      },
      "source": [
        "---\n",
        "\n",
        "# Net dissection on VGG \n",
        "\n",
        "In this notebook we want to share the process of net dissection on a particular convolutional neural network architecture. In this case we are going to work in VGG16, a renowned CNN that won the ILSVRC in 2014 a classification challenge with the imageNet dataset, we want to understand the role of units of this CNN in terms of human concepts of the classification process also we want to provide some interpretabilty for this modet, that is why we want to do a netdissect on VGG16."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To start we use a modification of net dissect library who the authors of the paper did, to make that works with the actual versions we fork the github repository and fix the compatibility problems, for this reason we clone our own version of net dissect library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbQ5DIAVwdWg",
        "outputId": "ddd3b7ef-2fb3-4e44-8d53-e76f69b3d736"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into 'global-model-repr'...\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "git clone -b main https://github.com/Robert-Gomez-DS/global-model-repr.git "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2xm2ZAjxwdWj"
      },
      "outputs": [],
      "source": [
        "try: # set up path\n",
        "    import google.colab, sys, torch\n",
        "    sys.path.append('/content/tutorial_code')\n",
        "    sys.path.append('./global-model-repr/') \n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"Change runtime type to include a GPU.\")  \n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JTZBM8VkwdWk"
      },
      "outputs": [],
      "source": [
        "import sys, scipy \n",
        "sys.path.append('./global-model-repr/') "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we import our version of netdissection,torch to use pytorch, also we import transformers to use an simple segmenter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1iZvpfnwdWm",
        "outputId": "4736fb75-f125-4624-f5bf-226cce4b64bb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/./global-model-repr/netdissect/segmenter.py:325: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "  assert result is not 0, 'unrecognized class %d' % classnum\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x7f1f67175270>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch, os, matplotlib.pyplot as plt\n",
        "from transformers import pipeline\n",
        "from netdissect import nethook, imgviz, show, segmenter, renormalize, upsample, tally, pbar\n",
        "from netdissect import setting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.set_grad_enabled(False) "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## VGG-16\n",
        "\n",
        "Now we download a dataset for which we use VGG16 to classify and who allows us to dissect the network.\n",
        "\n",
        "**What is VGG16**\n",
        "\n",
        "VGG16 is a convolutional neural network (CNN) architecture that was proposed by the Visual Geometry Group (VGG) at the University of Oxford. It gained significant popularity and achieved outstanding performance in image classification tasks, particularly in the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) in 2014.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<div style=\"display: flex; width: 100%;\">\n",
        "  <div style=\"flex: 4;width:10%;padding-left: 10px;\">\n",
        "      The architecture of VGG16 consists of 16 layers, hence the name. It follows a straightforward design principle of using small-sized convolutional filters (3x3) throughout the network, stacked together to form deeper representations. This uniformity in filter size allows for a deeper network with a simpler structure.\n",
        "  </div>\n",
        "  <div style=\"flex: 6;width:90%; padding-right: 10px;\">\n",
        "    <img src=\"illu_VGG-02.png\" alt=\"Descripción de la imagen\" style=\"width: 100%;\">\n",
        "  </div>\n",
        "  \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvYW3mSEwdWn"
      },
      "outputs": [],
      "source": [
        "ds = setting.load_dataset('places', 'val')\n",
        "iv = imgviz.ImageVisualizer(224, source=ds, percent_level=0.99)\n",
        "show(iv.image(ds[0][0]))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we are going to load the model VGG16 with architecture and weights , this is for study and interpretabilty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G85Fov37wdWp"
      },
      "outputs": [],
      "source": [
        "model = setting.load_vgg16()\n",
        "model = nethook.InstrumentedModel(model)\n",
        "\n",
        "renorm = renormalize.renormalizer(source=ds, target='zc')\n",
        "ivsmall = imgviz.ImageVisualizer((56, 56), source=ds, percent_level=0.99)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we want to print the architecture of VGG16 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Qd6TGYKwdWq"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we want to introduce VGG16 and how the paper works with it, our objective is to extract semantic information about units in the neural network,the paper shows that "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We want to see the predictions that the model do with the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09YB7ioqwdWs"
      },
      "outputs": [],
      "source": [
        "target_class = ds.classes.index('soccer_field')\n",
        "print(target_class)\n",
        "indexes = range(100, 112)\n",
        "batch = torch.stack([ds[i][0] for i in indexes])\n",
        "_, preds = model(batch.cuda()).max(1)\n",
        "show([[\n",
        "    iv.image(batch[j]),\n",
        "    'label: ' + ds.classes[ds[i][1]],\n",
        "    'pred: ' + ds.classes[preds[j]],\n",
        "    i,\n",
        "] for j, i in enumerate(indexes)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meEyq75GwdWt"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    correct = 0\n",
        "    tested = 0\n",
        "    for imagebatch, labelbatch in pbar(torch.utils.data.DataLoader(ds, batch_size=100)):\n",
        "        modelpreds = model(imagebatch).max(1)[1]\n",
        "        print(modelpreds.cpu(), labelbatch)\n",
        "        correct += (modelpreds.cpu() == labelbatch).sum() # fixme\n",
        "        tested += len(labelbatch)\n",
        "    print('%d correct out of %d' % (correct, tested))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are going to work with the layer 4 this is because the paper show that is the layer who activates more in the regions of objects so is more visibile for our case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iy12aWYJwdWu"
      },
      "outputs": [],
      "source": [
        "layername = 'features.conv4_3'\n",
        "model.retain_layer(layername)\n",
        "model(batch)\n",
        "acts = model.retained_layer(layername).cpu()\n",
        "print(acts.shape)\n",
        "for i in range(7):\n",
        "    show([\n",
        "    [\n",
        "        [ivsmall.masked_image(batch[imagenum], acts[imagenum], unitnum)],\n",
        "        [ivsmall.heatmap(acts[imagenum], unitnum, mode='nearest')],\n",
        "        'unit %d' % unitnum\n",
        "    ]\n",
        "    for unitnum in range(1,acts.shape[1]//10 -2,2)\n",
        "    \n",
        "    for imagenum in [i]\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V50L2OFbwdWv"
      },
      "outputs": [],
      "source": [
        "layername = 'features.conv4_3'\n",
        "model.retain_layer(layername)\n",
        "model(batch)\n",
        "acts = model.retained_layer(layername).cpu()\n",
        "print(acts.shape)\n",
        "for i in range(7):\n",
        "    show([\n",
        "    [\n",
        "        [ivsmall.masked_image(batch[imagenum], acts[imagenum], unitnum)],\n",
        "        [ivsmall.heatmap(acts[imagenum], unitnum, mode='nearest')],\n",
        "        'unit %d' % unitnum\n",
        "    ]\n",
        "    for unitnum in [9,35,298,315]\n",
        "    \n",
        "    for imagenum in [i]\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwbeJtLiwdWw"
      },
      "outputs": [],
      "source": [
        "upfn = upsample.upsampler(\n",
        "    target_shape=(56, 56),\n",
        "    data_shape=(7, 7),\n",
        ")\n",
        "\n",
        "def flatten_activations(batch, *args):\n",
        "    image_batch = batch.cuda()\n",
        "    _ = model(image_batch)\n",
        "    acts = model.retained_layer(layername)\n",
        "    hacts = upfn(acts)\n",
        "    return hacts.permute(0, 2, 3, 1).contiguous().view(-1, acts.shape[1])\n",
        "\n",
        "rq = tally.tally_quantile(\n",
        "    flatten_activations,\n",
        "    dataset=ds,\n",
        "    sample_size=500,\n",
        "    batch_size=50,\n",
        "    cachefile='results/rq_cache.npz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SFSEhvSt-z63"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'rq' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m plt\u001b[39m.\u001b[39mplot(rq\u001b[39m.\u001b[39mquantiles(\u001b[39m0.9\u001b[39m))\n",
            "\u001b[0;31mNameError\u001b[0m: name 'rq' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "plt.plot(rq.quantiles(0.9))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Examine images that activate most each unit\n",
        "\n",
        "Given a sample $n$ the next loop identifies the unit who activate strongest with some image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3WVVWNR-676"
      },
      "outputs": [],
      "source": [
        "sample_size = 1000\n",
        "\n",
        "def max_activations(batch, *args):\n",
        "    image_batch = batch.cuda()\n",
        "    _ = model(image_batch)\n",
        "    acts = model.retained_layer(layername)\n",
        "    return acts.view(acts.shape[:2] + (-1,)).max(2)[0]\n",
        "\n",
        "def mean_activations(batch, *args):\n",
        "    image_batch = batch.cuda()\n",
        "    _ = model(image_batch)\n",
        "    acts = model.retained_layer(layername)\n",
        "    return acts.view(acts.shape[:2] + (-1,)).mean(2)\n",
        "\n",
        "topk = tally.tally_topk(\n",
        "    mean_activations,\n",
        "    dataset=ds,\n",
        "    sample_size=sample_size,\n",
        "    batch_size=100,\n",
        "    cachefile='results/cache_mean_topk.npz'\n",
        ")\n",
        "\n",
        "top_indexes = topk.result()[1]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we run a loop that runs the model for each of the top-activating images for the unit $u$ and show where the unit activates within each image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGEN9Wyn_AGt"
      },
      "outputs": [],
      "source": [
        "show.blocks([\n",
        "    ['unit %d' % u,\n",
        "     'img %d' % i,\n",
        "     'pred: %s' % ds.classes[model(ds[i][0][None].cuda()).max(1)[1].item()],\n",
        "     [iv.masked_image(\n",
        "        ds[i][0],\n",
        "        model.retained_layer(layername)[0],\n",
        "        u)]\n",
        "    ]\n",
        "    for u in [12]\n",
        "    for i in top_indexes[u, :20]\n",
        "])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code automates the above process for all the units, collecting a visualization of top images for each unit in the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pZ26K07_DKI"
      },
      "outputs": [],
      "source": [
        "def compute_activations(image_batch, label_batch):\n",
        "    image_batch = image_batch.cuda()\n",
        "    _ = model(image_batch)\n",
        "    acts_batch = model.retained_layer(layername)\n",
        "    return acts_batch\n",
        "\n",
        "unit_images = iv.masked_images_for_topk(\n",
        "    compute_activations,\n",
        "    ds,\n",
        "    topk,\n",
        "    k=5,\n",
        "    num_workers=10,\n",
        "    pin_memory=True,\n",
        "    cachefile='results/cache_top10images.npz')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we introduce a segmentation algorithm to make the operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qfpfvyf_PDqh"
      },
      "outputs": [],
      "source": [
        " segmenter = pipeline(\"image-segmentation\", model=\"my_awesome_seg_model\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
